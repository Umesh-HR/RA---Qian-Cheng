{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processed 3 bands a time, and collected multiple images, using various strategies to combine them:\n",
    "\n",
    "-\tMajority Voting: Assign each pixel the most frequently predicted class from different band combinations.  \n",
    "-\tWeighted Averaging: Give more importance to specific bands or results with higher confidence scores.  \n",
    "-\tFeature Fusion via CNN or MLP: Use a post-processing model to learn the optimal way to fuse predictions from different band combinations.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# Load your four segmentation images (binary masks)\n",
    "common_path = '/Users/seaqueue/Documents/AI/Research/blueberry/understand_HSI/'\n",
    "paths = [common_path+'dct_012.png', common_path+'dct_024.png', common_path+'dct_034.png', common_path+'dct_123.png']\n",
    "\n",
    "# Load images and convert them into binary arrays\n",
    "masks = [np.array(Image.open(p).convert('L')) for p in paths]\n",
    "\n",
    "# Threshold images to binary values (assuming 0 and 255)\n",
    "binary_masks = [(mask > 127).astype(np.uint8) for mask in masks]\n",
    "\n",
    "# Stack masks along a new dimension\n",
    "stacked_masks = np.stack(binary_masks, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform majority voting\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "vote_sum = np.sum(stacked_masks, axis=0)\n",
    "majority_vote = (vote_sum >= 3).astype(np.uint8) * 255  # 3 or more votes out of 4\n",
    "# Save the resulting image\n",
    "cv2.imwrite('/Users/seaqueue/Documents/AI/Research/blueberry/understand_HSI/merged_majority_vote.png', majority_vote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform Average-based Fusion\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "avg_mask = np.mean(stacked_masks, axis=0)\n",
    "soft_vote = (avg_mask > 0.5).astype(np.uint8) * 255\n",
    "cv2.imwrite('/Users/seaqueue/Documents/AI/Research/blueberry/understand_HSI/merged_soft_vote.png', soft_vote)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Intersection (Logical AND)keeps pixels segmented by all models.\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "intersection_mask = np.all(stacked_masks, axis=0).astype(np.uint8) * 255\n",
    "cv2.imwrite('/Users/seaqueue/Documents/AI/Research/blueberry/understand_HSI/merged_intersection.png', intersection_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Union (Logical OR) Pixels segmented by any model.\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "union_mask = np.any(stacked_masks, axis=0).astype(np.uint8) * 255\n",
    "cv2.imwrite('/Users/seaqueue/Documents/AI/Research/blueberry/understand_HSI/merged_union.png', union_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Weighted Average\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "# Define weights for each model (must sum up to 1.0 ideally)\n",
    "weights = [0.4, 0.3, 0.2, 0.1]\n",
    "\n",
    "# Compute weighted average\n",
    "weighted_sum = np.zeros_like(binary_masks[0], dtype=np.float32)\n",
    "for mask, weight in zip(binary_masks, weights):\n",
    "    weighted_sum += mask * weight\n",
    "\n",
    "# Threshold the weighted sum at 0.5 to obtain final segmentation\n",
    "weighted_avg_result = (weighted_sum >= 0.5).astype(np.uint8) * 255\n",
    "# Save result\n",
    "cv2.imwrite('/Users/seaqueue/Documents/AI/Research/blueberry/understand_HSI/weighted_average_result.png', weighted_avg_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature fusion typically involves combining multiple segmentation predictions at the feature-level \n",
    "# (e.g., confidence maps or logits) rather than binary outputs. \n",
    "# If your outputs are binary masks, feature fusion would be limited. \n",
    "# For richer feature fusion, you'd ideally have probability maps or feature maps directly from the model outputs.\n",
    "# Max Fusion: Take pixel-wise max of all segmentation maps.\n",
    "max_fusion = np.max(np.stack(binary_masks, axis=0), axis=0)\n",
    "max_fusion_result = (max_fusion > 0).astype(np.uint8) * 255\n",
    "cv2.imwrite('max_feature_fusion.png', max_fusion_result)\n",
    "\n",
    "\n",
    "# Max Fusion: Take pixel-wise max of all segmentation maps.\n",
    "multiplicative_fusion = np.prod(np.stack(binary_masks, axis=0), axis=0)\n",
    "multiplicative_result = (multiplicative_fusion > 0).astype(np.uint8) * 255\n",
    "cv2.imwrite('multiplicative_fusion.png', multiplicative_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine RGB to get the true color\n",
    "![multispectral camera setup](camera_setup.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check band's spatial resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Band: /Users/seaqueue/Documents/AI/Research/blueberry/understand_HSI/IMG_0200_1.tif\n",
      "Resolution: (1.0, 1.0)\n",
      "Shape: 1280 x 960\n",
      "Band: /Users/seaqueue/Documents/AI/Research/blueberry/understand_HSI/IMG_0200_2.tif\n",
      "Resolution: (1.0, 1.0)\n",
      "Shape: 1280 x 960\n",
      "Band: /Users/seaqueue/Documents/AI/Research/blueberry/understand_HSI/IMG_0200_3.tif\n",
      "Resolution: (1.0, 1.0)\n",
      "Shape: 1280 x 960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/hsi/lib/python3.10/site-packages/rasterio/__init__.py:356: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import rasterio\n",
    "\n",
    "with rasterio.open(\"/Users/seaqueue/Documents/AI/Research/blueberry/understand_HSI/IMG_0200_1.tif\") as src:\n",
    "    print(f\"Band: {src.name}\")\n",
    "    print(f\"Resolution: {src.res}\")   # (pixel width, pixel height)\n",
    "    print(f\"Shape: {src.width} x {src.height}\")\n",
    "    \n",
    "with rasterio.open(\"/Users/seaqueue/Documents/AI/Research/blueberry/understand_HSI/IMG_0200_2.tif\") as src:\n",
    "    print(f\"Band: {src.name}\")\n",
    "    print(f\"Resolution: {src.res}\")   # (pixel width, pixel height)\n",
    "    print(f\"Shape: {src.width} x {src.height}\")\n",
    "    \n",
    "with rasterio.open(\"/Users/seaqueue/Documents/AI/Research/blueberry/understand_HSI/IMG_0200_3.tif\") as src:\n",
    "    print(f\"Band: {src.name}\")\n",
    "    print(f\"Resolution: {src.res}\")   # (pixel width, pixel height)\n",
    "    print(f\"Shape: {src.width} x {src.height}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RGB bands are slightly offset. Need to align them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved aligned RGB image to: red_edge_aligned_rgb.png\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# -------- CONFIG --------\n",
    "# red_path = 'Metashape_aligned/IMG_0200_3.tif'\n",
    "# green_path = 'Metashape_aligned/IMG_0200_2.tif'\n",
    "# blue_path = 'Metashape_aligned/IMG_0200_1.tif'\n",
    "# nir_path = 'Metashape_aligned/IMG_0200_4.tif'\n",
    "# red_edge_path = 'Metashape_aligned/IMG_0200_5.tif'\n",
    "# output_path = 'Metashape_aligned/red_edge_aligned_rgb.png'\n",
    "\n",
    "red_path = 'IMG_0200_3.tif'\n",
    "green_path = 'IMG_0200_2.tif'\n",
    "blue_path = 'IMG_0200_1.tif'\n",
    "nir_path = 'IMG_0200_4.tif'\n",
    "red_edge_path = 'IMG_0200_5.tif'\n",
    "output_path = 'red_edge_aligned_rgb.png'\n",
    "# ------------------------\n",
    "\n",
    "# Load 16-bit images\n",
    "red = cv2.imread(red_path, cv2.IMREAD_UNCHANGED)\n",
    "green = cv2.imread(green_path, cv2.IMREAD_UNCHANGED)\n",
    "blue = cv2.imread(blue_path, cv2.IMREAD_UNCHANGED)\n",
    "nir = cv2.imread(nir_path, cv2.IMREAD_UNCHANGED)\n",
    "red_edge = cv2.imread(red_edge_path, cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "# Alignment function\n",
    "def align_images(base, to_align):\n",
    "    warp_matrix = np.eye(2, 3, dtype=np.float32)\n",
    "    criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 5000, 1e-10)\n",
    "\n",
    "    try:\n",
    "        cc, warp_matrix = cv2.findTransformECC(\n",
    "            cv2.normalize(base, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8),\n",
    "            cv2.normalize(to_align, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8),\n",
    "            warp_matrix,\n",
    "            cv2.MOTION_TRANSLATION,\n",
    "            criteria\n",
    "        )\n",
    "    except:\n",
    "        print(\"Alignment failed, using original image\")\n",
    "        return to_align\n",
    "\n",
    "    aligned = cv2.warpAffine(to_align, warp_matrix, (base.shape[1], base.shape[0]), flags=cv2.INTER_LINEAR + cv2.WARP_INVERSE_MAP)\n",
    "    return aligned\n",
    "\n",
    "# Align bands to red\n",
    "red_aligned = align_images(red_edge, red)\n",
    "green_aligned = align_images(red_edge, green)\n",
    "blue_aligned = align_images(red_edge, blue)\n",
    "\n",
    "# Normalize for PNG (convert 16-bit to 8-bit)\n",
    "def normalize_to_uint8(img):\n",
    "    norm = cv2.normalize(img, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    return norm.astype(np.uint8)\n",
    "\n",
    "red_8 = normalize_to_uint8(red_aligned)\n",
    "green_8 = normalize_to_uint8(green_aligned)\n",
    "blue_8 = normalize_to_uint8(blue_aligned)\n",
    "\n",
    "# Merge and save\n",
    "rgb_8bit = cv2.merge([blue_8, green_8, red_8])\n",
    "cv2.imwrite(output_path, rgb_8bit)\n",
    "\n",
    "print(f\"Saved aligned RGB image to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automated Feature-based alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aligned RGB composite saved as 'nir_feature_aligned_rgb_2.png'.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def align_image(ref_path, src_path, detector=None):\n",
    "    \"\"\"\n",
    "    Align the source image to the reference image using ORB feature matching\n",
    "    and homography estimation.\n",
    "\n",
    "    Parameters:\n",
    "      ref_path (str): File path to the reference image.\n",
    "      src_path (str): File path to the source image to be aligned.\n",
    "      detector (cv2.Feature2D, optional): Feature detector (e.g., ORB). If None, a default ORB detector is created.\n",
    "\n",
    "    Returns:\n",
    "      aligned_src (np.ndarray): The aligned version of the source image.\n",
    "    \"\"\"\n",
    "    # Load images in grayscale\n",
    "    ref_img = cv2.imread(ref_path, cv2.IMREAD_GRAYSCALE)\n",
    "    src_img = cv2.imread(src_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    if ref_img is None:\n",
    "        raise IOError(f\"Reference image '{ref_path}' could not be loaded.\")\n",
    "    if src_img is None:\n",
    "        raise IOError(f\"Source image '{src_path}' could not be loaded.\")\n",
    "\n",
    "    # Use provided detector or create one.\n",
    "    if detector is None:\n",
    "        detector = cv2.ORB_create(500)\n",
    "\n",
    "    # Detect keypoints and compute descriptors for both images.\n",
    "    kp_ref, des_ref = detector.detectAndCompute(ref_img, None)\n",
    "    kp_src, des_src = detector.detectAndCompute(src_img, None)\n",
    "\n",
    "    # Create the brute-force matcher and match descriptors.\n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "    matches = bf.match(des_ref, des_src)\n",
    "    matches = sorted(matches, key=lambda x: x.distance)\n",
    "\n",
    "    # Extract the coordinates for the best matches.\n",
    "    pts_ref = np.float32([kp_ref[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "    pts_src = np.float32([kp_src[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "\n",
    "    # Compute the homography matrix using RANSAC.\n",
    "    H, status = cv2.findHomography(pts_src, pts_ref, cv2.RANSAC, 5.0)\n",
    "    if H is None:\n",
    "        raise ValueError(f\"Homography could not be computed between '{ref_path}' and '{src_path}'.\")\n",
    "\n",
    "    # Warp the source image to align with the reference image.\n",
    "    aligned_src = cv2.warpPerspective(src_img, H, (ref_img.shape[1], ref_img.shape[0]))\n",
    "\n",
    "    return aligned_src\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # -------- CONFIG --------\n",
    "    red_path = 'IMG_0200_3.tif'\n",
    "    green_path = 'IMG_0200_2.tif'\n",
    "    blue_path = 'IMG_0200_1.tif'\n",
    "    nir_path = 'IMG_0200_4.tif'\n",
    "    red_edge_path = 'IMG_0200_5.tif'\n",
    "    output_path = 'red_edge_aligned_rgb.png'\n",
    "\n",
    "    # Create a shared ORB detector (optional but ensures consistency between alignments).\n",
    "    orb_detector = cv2.ORB_create(500)\n",
    "\n",
    "    # Align each RGB band to the red_edge reference.\n",
    "    aligned_red   = align_image(nir_path, red_path, detector=orb_detector)\n",
    "    aligned_green = align_image(nir_path, green_path, detector=orb_detector)\n",
    "    aligned_blue  = align_image(nir_path, blue_path, detector=orb_detector)\n",
    "\n",
    "    # Merge channels to create an RGB composite.\n",
    "    # Note: OpenCV uses channel order [Blue, Green, Red].\n",
    "    rgb_composite = cv2.merge([aligned_blue, aligned_green, aligned_red])\n",
    "\n",
    "    # Save the final composite image.\n",
    "    out_rgb_filename = 'nir_feature_aligned_rgb_2.png'\n",
    "    cv2.imwrite(out_rgb_filename, rgb_composite)\n",
    "    print(f\"Aligned RGB composite saved as '{out_rgb_filename}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine one image of 3 brands "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Paths to your individual band files\n",
    "red_path = 'IMG_0200_3.tif'\n",
    "green_path = 'IMG_0200_2.tif'\n",
    "blue_path = 'IMG_0200_1.tif'\n",
    "\n",
    "# Read each band\n",
    "with rasterio.open(red_path) as red_src:\n",
    "    red = red_src.read(1)\n",
    "    profile = red_src.profile  # To use for output if needed\n",
    "\n",
    "with rasterio.open(green_path) as green_src:\n",
    "    green = green_src.read(1)\n",
    "\n",
    "with rasterio.open(blue_path) as blue_src:\n",
    "    blue = blue_src.read(1)\n",
    "\n",
    "# Stack into RGB\n",
    "rgb = np.stack([red, green, blue], axis=-1)\n",
    "\n",
    "# Optional: Normalize to 0–255 if not already in that range\n",
    "rgb_normalized = (255 * (rgb / rgb.max())).astype(np.uint8)\n",
    "\n",
    "# Save as PNG image\n",
    "Image.fromarray(rgb_normalized).save('0200_rgb.png')\n",
    "\n",
    "# OR — Save as a new multi-band GeoTIFF\n",
    "# with rasterio.open('output_rgb.tif', 'w', **profile) as dst:\n",
    "#     dst.write(rgb[:, :, 0], 1)  # R\n",
    "#     dst.write(rgb[:, :, 1], 2)  # G\n",
    "#     dst.write(rgb[:, :, 2], 3)  # B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### combine all the images of 3 bands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from PIL import Image\n",
    "\n",
    "# Set the folder path where the .tif files are stored\n",
    "folder_path = \"/Users/seaqueue/Downloads/blueberry/batch1/\"  # change this to your folder\n",
    "\n",
    "# Find all .tif files in the folder\n",
    "file_list = glob.glob(os.path.join(folder_path, \"IMG_*_*.tif\"))\n",
    "\n",
    "# Dictionary to store files by location\n",
    "locations = {}\n",
    "\n",
    "for filepath in file_list:\n",
    "    filename = os.path.basename(filepath)\n",
    "    # Expecting a pattern like \"IMG_0200_1.tif\"\n",
    "    try:\n",
    "        # Split filename: \"IMG\", \"0200\", \"1.tif\"\n",
    "        parts = filename.split(\"_\")\n",
    "        location = \"_\".join(parts[:2])  # e.g., \"IMG_0200\"\n",
    "        band_str = parts[-1].split('.')[0]  # e.g., \"1\"\n",
    "    except IndexError:\n",
    "        print(f\"Skipping file with unexpected format: {filename}\")\n",
    "        continue\n",
    "\n",
    "    if location not in locations:\n",
    "        locations[location] = {}\n",
    "    locations[location][band_str] = filepath\n",
    "\n",
    "# Process each location\n",
    "for loc, bands in locations.items():\n",
    "    # print(loc, bands)\n",
    "    # Check if bands 1, 2, and 3 exist\n",
    "    if all(b in bands for b in ['1', '2', '3']):\n",
    "        # Read each band using rasterio\n",
    "        with rasterio.open(bands['1']) as src:\n",
    "            blue = src.read(1)\n",
    "        with rasterio.open(bands['2']) as src:\n",
    "            green = src.read(1)\n",
    "        with rasterio.open(bands['3']) as src:\n",
    "            red = src.read(1)\n",
    "        \n",
    "        # Stack the bands into an (height, width, 3) array in RGB order\n",
    "        # Note: We are mapping band 1 -> Blue, band 2 -> Green, band 3 -> Red.\n",
    "        rgb_array = np.dstack((red, green, blue))\n",
    "        \n",
    "        rgb_normalized = (255 * (rgb_array / rgb_array.max())).astype(np.uint8)\n",
    "        image = Image.fromarray(rgb_normalized)\n",
    "        # image = Image.fromarray(rgb_array.astype(np.uint8))\n",
    "        output_filename = os.path.join(\"/Users/seaqueue/Downloads/blueberry/batch1_rgb/\", f\"{loc}_rgb.png\")\n",
    "        image.save(output_filename)\n",
    "        print(f\"Saved RGB image for {loc} as {output_filename}\")\n",
    "    else:\n",
    "        print(f\"Missing required bands for location {loc}, skipping.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hsi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
